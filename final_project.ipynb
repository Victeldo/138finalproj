{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-17T03:41:54.535726Z",
     "start_time": "2023-12-17T03:41:47.496390Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /Users/prithvi/anaconda3/lib/python3.11/site-packages (2.15.0)\r\n",
      "Requirement already satisfied: gym in /Users/prithvi/anaconda3/lib/python3.11/site-packages (0.26.2)\r\n",
      "Requirement already satisfied: keras-rl in /Users/prithvi/anaconda3/lib/python3.11/site-packages (0.4.2)\r\n",
      "Requirement already satisfied: tensorflow-macos==2.15.0 in /Users/prithvi/anaconda3/lib/python3.11/site-packages (from tensorflow) (2.15.0)\r\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/prithvi/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (2.0.0)\r\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/prithvi/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (1.6.3)\r\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /Users/prithvi/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (23.5.26)\r\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Users/prithvi/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (0.5.4)\r\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/prithvi/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (0.2.0)\r\n",
      "Requirement already satisfied: h5py>=2.9.0 in /Users/prithvi/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (3.9.0)\r\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/prithvi/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (16.0.6)\r\n",
      "Requirement already satisfied: ml-dtypes~=0.2.0 in /Users/prithvi/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (0.2.0)\r\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /Users/prithvi/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (1.24.3)\r\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/prithvi/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (3.3.0)\r\n",
      "Requirement already satisfied: packaging in /Users/prithvi/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (23.1)\r\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /Users/prithvi/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (4.23.4)\r\n",
      "Requirement already satisfied: setuptools in /Users/prithvi/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (68.0.0)\r\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/prithvi/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (1.16.0)\r\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/prithvi/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (2.4.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/prithvi/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (4.7.1)\r\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /Users/prithvi/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (1.14.1)\r\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Users/prithvi/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (0.34.0)\r\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/prithvi/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (1.60.0)\r\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in /Users/prithvi/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (2.15.1)\r\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /Users/prithvi/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (2.15.0)\r\n",
      "Requirement already satisfied: keras<2.16,>=2.15.0 in /Users/prithvi/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (2.15.0)\r\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /Users/prithvi/anaconda3/lib/python3.11/site-packages (from gym) (2.2.1)\r\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /Users/prithvi/anaconda3/lib/python3.11/site-packages (from gym) (0.0.8)\r\n",
      "Requirement already satisfied: ale-py~=0.8.0 in /Users/prithvi/anaconda3/lib/python3.11/site-packages (from gym) (0.8.1)\r\n",
      "Collecting autorom[accept-rom-license]~=0.4.2 (from gym)\r\n",
      "  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\r\n",
      "Requirement already satisfied: importlib-resources in /Users/prithvi/anaconda3/lib/python3.11/site-packages (from ale-py~=0.8.0->gym) (6.1.1)\r\n",
      "Requirement already satisfied: click in /Users/prithvi/anaconda3/lib/python3.11/site-packages (from autorom[accept-rom-license]~=0.4.2->gym) (8.0.4)\r\n",
      "Requirement already satisfied: requests in /Users/prithvi/anaconda3/lib/python3.11/site-packages (from autorom[accept-rom-license]~=0.4.2->gym) (2.31.0)\r\n",
      "Requirement already satisfied: tqdm in /Users/prithvi/anaconda3/lib/python3.11/site-packages (from autorom[accept-rom-license]~=0.4.2->gym) (4.65.0)\r\n",
      "Collecting AutoROM.accept-rom-license (from autorom[accept-rom-license]~=0.4.2->gym)\r\n",
      "  Downloading AutoROM.accept-rom-license-0.6.1.tar.gz (434 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m434.7/434.7 kB\u001B[0m \u001B[31m13.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25h  Installing build dependencies ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Getting requirements to build wheel ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Preparing metadata (pyproject.toml) ... \u001B[?25ldone\r\n",
      "\u001B[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /Users/prithvi/anaconda3/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow-macos==2.15.0->tensorflow) (0.38.4)\r\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/prithvi/anaconda3/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (2.25.2)\r\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /Users/prithvi/anaconda3/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (1.2.0)\r\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/prithvi/anaconda3/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (3.4.1)\r\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/prithvi/anaconda3/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (0.7.2)\r\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/prithvi/anaconda3/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (2.2.3)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/prithvi/anaconda3/lib/python3.11/site-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym) (2.0.4)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/prithvi/anaconda3/lib/python3.11/site-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/prithvi/anaconda3/lib/python3.11/site-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym) (1.26.16)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/prithvi/anaconda3/lib/python3.11/site-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym) (2023.7.22)\r\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/prithvi/anaconda3/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (5.3.2)\r\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/prithvi/anaconda3/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (0.2.8)\r\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/prithvi/anaconda3/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (4.9)\r\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/prithvi/anaconda3/lib/python3.11/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (1.3.1)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/prithvi/anaconda3/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (2.1.1)\r\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/prithvi/anaconda3/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (0.4.8)\r\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/prithvi/anaconda3/lib/python3.11/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (3.2.2)\r\n",
      "Building wheels for collected packages: AutoROM.accept-rom-license\r\n",
      "  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.6.1-py3-none-any.whl size=446660 sha256=d8b3cc73d5d1a1711349aeee23ae1dff48990bb1d37c377e7ae628f64c739cd5\r\n",
      "  Stored in directory: /Users/prithvi/Library/Caches/pip/wheels/bc/fc/c6/8aa657c0d2089982f2dabd110efc68c61eb49831fdb7397351\r\n",
      "Successfully built AutoROM.accept-rom-license\r\n",
      "Installing collected packages: AutoROM.accept-rom-license, autorom\r\n",
      "Successfully installed AutoROM.accept-rom-license-0.6.1 autorom-0.4.2\r\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow gym keras-rl 'gym[atari]' 'gym[accept-rom-license]' opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-17T04:01:05.567558Z",
     "start_time": "2023-12-17T04:01:02.585397Z"
    }
   },
   "outputs": [],
   "source": [
    "import gym \n",
    "import random\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-17T03:42:07.862848Z",
     "start_time": "2023-12-17T03:42:06.409363Z"
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make('Breakout-v4', render_mode='human')\n",
    "height, width, channels = env.observation_space.shape\n",
    "actions = env.action_space.n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-17T03:42:09.610094Z",
     "start_time": "2023-12-17T03:42:09.605764Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['NOOP', 'FIRE', 'RIGHT', 'LEFT']"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.unwrapped.get_action_meanings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-17T03:42:53.100370Z",
     "start_time": "2023-12-17T03:42:12.554355Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/prithvi/anaconda3/lib/python3.11/site-packages/gym/utils/passive_env_checker.py:289: UserWarning: \u001B[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001B[0m\n",
      "  logger.warn(\n",
      "/Users/prithvi/anaconda3/lib/python3.11/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 Score: 2.0\n",
      "Episode: 1 Score: 3.0\n",
      "Episode: 2 Score: 0.0\n",
      "Episode: 3 Score: 3.0\n",
      "Episode: 4 Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Breakout-v4', render_mode='human')\n",
    "episodes = 5 \n",
    "for episode in range(episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    while not done: \n",
    "        env.render()\n",
    "        action = random.choice([0,1,2,3])\n",
    "        n_state, reward, done, truncated, info = env.step(action)\n",
    "        score += reward\n",
    "    print(\"Episode:\", episode, \"Score:\", score)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-17T03:43:11.966751Z",
     "start_time": "2023-12-17T03:42:57.542894Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(3444837047, 2669555309)"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Configuration paramaters for the whole setup\n",
    "seed = 42\n",
    "gamma = 0.99  # Discount factor for past rewards\n",
    "epsilon = 1.0  # Epsilon greedy parameter\n",
    "epsilon_min = 0.1  # Minimum epsilon greedy parameter\n",
    "epsilon_max = 1.0  # Maximum epsilon greedy parameter\n",
    "epsilon_interval = (\n",
    "    epsilon_max - epsilon_min\n",
    ")  # Rate at which to reduce chance of random action being taken\n",
    "batch_size = 32  # Size of batch taken from replay buffer\n",
    "max_steps_per_episode = 10000\n",
    "\n",
    "# Use the Baseline Atari environment because of Deepmind helper functions\n",
    "env = gym.make(\"Breakout-v4\")\n",
    "env.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-17T03:43:16.953493Z",
     "start_time": "2023-12-17T03:43:16.824130Z"
    }
   },
   "outputs": [],
   "source": [
    "num_actions = 4\n",
    "\n",
    "\n",
    "def create_q_model():\n",
    "    # Network defined by the Deepmind paper\n",
    "    inputs = layers.Input(shape=(84, 84, 4,))\n",
    "\n",
    "    # Convolutions on the frames on the screen\n",
    "    layer1 = layers.Conv2D(32, 8, strides=4, activation=\"relu\")(inputs)\n",
    "    layer2 = layers.Conv2D(64, 4, strides=2, activation=\"relu\")(layer1)\n",
    "    layer3 = layers.Conv2D(64, 3, strides=1, activation=\"relu\")(layer2)\n",
    "\n",
    "    layer4 = layers.Flatten()(layer3)\n",
    "\n",
    "    layer5 = layers.Dense(512, activation=\"relu\")(layer4)\n",
    "    action = layers.Dense(num_actions, activation=\"linear\")(layer5)\n",
    "\n",
    "    return keras.Model(inputs=inputs, outputs=action)\n",
    "\n",
    "\n",
    "# The first model makes the predictions for Q-values which are used to\n",
    "# make a action.\n",
    "model = create_q_model()\n",
    "# Build a target model for the prediction of future rewards.\n",
    "# The weights of a target model get updated every 10000 steps thus when the\n",
    "# loss between the Q-values is calculated the target Q-value is stable.\n",
    "model_target = create_q_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-17T03:43:22.098536Z",
     "start_time": "2023-12-17T03:43:22.090199Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Activation\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.optimizers import Adam\n",
    "import numpy as np\n",
    "\n",
    "class replayBuffer(object):\n",
    "    def __init__(self, max_size, input_shape, n_actions, discrete=False):\n",
    "        self.mem_size = max_size\n",
    "        self.input = input_shape\n",
    "        #dealing with discrete (1 hot) or continous, policy gradients\n",
    "        self.discrete = discrete \n",
    "        self.state_memory = np.zeros(((self.mem_size,) + input_shape))\n",
    "        self.new_state_memory = np.zeros(((self.mem_size,) + input_shape))\n",
    "        dtype = np.int8 if self.discrete else np.float32\n",
    "        self.action_memory = np.zeros((self.mem_size, n_actions))\n",
    "        self.reward_memory = np.zeros(self.mem_size)\n",
    "        self.terminal_memory = np.zeros(self.mem_size, dtype=np.float32)\n",
    "        self.mem_cntr = 0\n",
    "\n",
    "    def store_transition(self, state, action, reward, state_, done):\n",
    "        index = self.mem_cntr % self.mem_size\n",
    "        self.state_memory[index] = state[0]\n",
    "        self.new_state_memory[index] = state_[0]\n",
    "        self.reward_memory[index] = reward\n",
    "        self.terminal_memory[index] = 1 - int(done)\n",
    "        if self.discrete:\n",
    "            actions = np.zeros(self.action_memory.shape[1])\n",
    "            actions[action] = 1.0\n",
    "            self.action_memory[index] = actions\n",
    "        else:\n",
    "            self.action_memory[index] = action \n",
    "        self.mem_cntr += 1\n",
    "\n",
    "    def sample_buffer(self, batch_size):\n",
    "        max_mem = min(self.mem_cntr, self.mem_size)\n",
    "        batch = np.random.choice(max_mem, batch_size)\n",
    "\n",
    "        states = self.state_memory[batch]\n",
    "        states_ = self.new_state_memory[batch]\n",
    "        rewards = self.reward_memory[batch]\n",
    "        actions = self.action_memory[batch]\n",
    "        terminal = self.terminal_memory[batch]\n",
    "\n",
    "        return states, actions, rewards, states_, terminal\n",
    "    \n",
    "def build_dqn(lr, n_actions, input_dims, dcl_dims, fc2_dims):\n",
    "    model = Sequential([\n",
    "                Dense(dcl_dims, input_shape=input_dims),\n",
    "                Activation('relu'),\n",
    "                Dense(fc2_dims),\n",
    "                Activation('relu'),\n",
    "                Dense(n_actions)])\n",
    "\n",
    "\n",
    "class Agent(object):\n",
    "    def __init__(self, alpha, gamma, n_actions, epsilon, batch_size, input_dims, epsilon_dec=0.996, epsilon_end=0.01, mem_size=10000, fname='dqn_model.h5'):\n",
    "        self.action_space = [i for i in range(n_actions)]\n",
    "        self.n_actions = n_actions\n",
    "        self.gamma = gamma \n",
    "        self.epsilon = epsilon \n",
    "        self.epsilon_dec = epsilon_dec\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.batch_size = batch_size\n",
    "        self.model_file = fname\n",
    "\n",
    "        self.memory = replayBuffer(mem_size, input_dims, n_actions, discrete=True)\n",
    "\n",
    "        self.q_eval = build_dqn(alpha, n_actions, input_dims, 256, 256)\n",
    "\n",
    "    def remeber(self, state, action, reward, new_state, done):\n",
    "        self.memory.store_transition(state, action, reward, new_state, done)\n",
    "\n",
    "    def chose_action(self, state):\n",
    "        rand = np.random.random()\n",
    "        if rand < self.epsilon:\n",
    "            action = np.random.choice(self.action_space)\n",
    "        else:\n",
    "            actions = self.q_eval.predict(state)\n",
    "            action = np.argmax(actions)\n",
    "\n",
    "        return action\n",
    "        \n",
    "    def learn(self):\n",
    "        if self.memory.mem_cntr < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        #sample from the memory buffer, non-sequential memories (avoid correlations in learning)\n",
    "        state, action, reward, new_state, done = self.memory.sample_buffer(self.batch_size)\n",
    "        \n",
    "        #go from one hot encoding to integer\n",
    "        action_values = np.array(self.action_space, dtype=np.int8)\n",
    "        action_indices = np.dot(action, action_values)\n",
    "\n",
    "        q_eval = self.q_eval.predict(state)\n",
    "        q_next = self.q_eval.predict(new_state)\n",
    "\n",
    "        q_target = q_eval.copy()\n",
    "\n",
    "\n",
    "        batch_index = np.arange(self.batch_size, dtype=np.int32)\n",
    "\n",
    "        #update q targets based on max values of next state\n",
    "        print(\"q target is \",q_target)\n",
    "        print(\"q next is \", q_next)\n",
    "        q_target[batch_index, action_indices] = reward + self.gamma * np.max(q_next, axis=1) * done\n",
    "\n",
    "        #use q_target as target for loss function of q network\n",
    "        _ = self.q_eval.fit(state, q_target, verbose=0)\n",
    "\n",
    "        self.epsilon = self.epsilon*self.epsilon_dec if self.epsilon > self.epsilon_min else self.epsilon_min\n",
    "\n",
    "    def save_model(self):\n",
    "        self.q_eval.save(self.model_file)\n",
    "\n",
    "    def load_model(self):\n",
    "        self.q_eval = load_model(self.model_file)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-17T03:43:24.511659Z",
     "start_time": "2023-12-17T03:43:24.248261Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/prithvi/anaconda3/lib/python3.11/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[15], line 17\u001B[0m\n\u001B[1;32m     15\u001B[0m     agent\u001B[38;5;241m.\u001B[39mremeber(observation, action, reward, observation_, done)\n\u001B[1;32m     16\u001B[0m     observation \u001B[38;5;241m=\u001B[39m observation_\n\u001B[0;32m---> 17\u001B[0m     agent\u001B[38;5;241m.\u001B[39mlearn()\n\u001B[1;32m     19\u001B[0m eps_history\u001B[38;5;241m.\u001B[39mappend(agent\u001B[38;5;241m.\u001B[39mepsilon)\n\u001B[1;32m     20\u001B[0m scores\u001B[38;5;241m.\u001B[39mappend(score)\n",
      "Cell \u001B[0;32mIn[14], line 94\u001B[0m, in \u001B[0;36mAgent.learn\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     91\u001B[0m action_values \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maction_space, dtype\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39mint8)\n\u001B[1;32m     92\u001B[0m action_indices \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mdot(action, action_values)\n\u001B[0;32m---> 94\u001B[0m q_eval \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mq_eval\u001B[38;5;241m.\u001B[39mpredict(state)\n\u001B[1;32m     95\u001B[0m q_next \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mq_eval\u001B[38;5;241m.\u001B[39mpredict(new_state)\n\u001B[1;32m     97\u001B[0m q_target \u001B[38;5;241m=\u001B[39m q_eval\u001B[38;5;241m.\u001B[39mcopy()\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'NoneType' object has no attribute 'predict'"
     ]
    }
   ],
   "source": [
    "n_games = 20\n",
    "agent = Agent(gamma=.99, epsilon=1.0, alpha=.0005, n_actions=4, input_dims=(210, 160, 3) , mem_size=100, batch_size=64, epsilon_end=0.01)\n",
    "\n",
    "scores = []\n",
    "eps_history = []\n",
    "\n",
    "for i in range(n_games):\n",
    "    done = False\n",
    "    score = 0\n",
    "    observation = env.reset()\n",
    "    while not done: \n",
    "        action = agent.chose_action(observation)\n",
    "        observation_, reward, truncated, done, info = env.step(action)\n",
    "        score += reward\n",
    "        agent.remeber(observation, action, reward, observation_, done)\n",
    "        observation = observation_\n",
    "        agent.learn()\n",
    "\n",
    "    eps_history.append(agent.epsilon)\n",
    "    scores.append(score)\n",
    "\n",
    "    avg_score = np.mean(scores[max(0, 1 - 100):(i + 1)])\n",
    "    print(\"episode \", i, \"score %.2f\" % score, 'average score %.2f' % avg_score)\n",
    "\n",
    "    if i % 10 == 0 and i > 0: \n",
    "        agent.save_model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class For Memory Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-17T03:49:40.432389Z",
     "start_time": "2023-12-17T03:49:40.428957Z"
    }
   },
   "outputs": [],
   "source": [
    "class Memory(object):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        self.states = []\n",
    "        self.next_states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.terminal = []\n",
    "\n",
    "    def save_action(self, state, action, reward, next_state, done):\n",
    "        print(\"length of state in save actions is \", len(state))\n",
    "        self.states.append(state)\n",
    "        self.next_states.append(next_state)\n",
    "        self.actions.append(action)\n",
    "        self.rewards.append(reward)\n",
    "        self.terminal.append(1 - int(done))\n",
    "\n",
    "    def sample_memory(self):\n",
    "        return self.states, self.actions, self.rewards, self.next_states, self.terminal\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function For Deep Q Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-17T03:49:41.343511Z",
     "start_time": "2023-12-17T03:49:41.336333Z"
    }
   },
   "outputs": [],
   "source": [
    "def init_deep_q_network(input_shape, lr, num_actions, l1_size, l2_size):\n",
    "    model = Sequential([\n",
    "        Dense(l1_size, input_shape=input_shape),\n",
    "        Activation('relu'),\n",
    "        Dense(l2_size),\n",
    "        Activation('relu'),\n",
    "        Dense(num_actions)])\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=lr), loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class For Learning Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-17T03:49:42.669782Z",
     "start_time": "2023-12-17T03:49:42.667840Z"
    }
   },
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    def __init__(self, alpha, gamma, actions_amt, epsilon, batch_size, input_shape, epsilon_dec=0.996, epsilon_end=0.01, mem_size=10000, fname='dqn_model.h5'):\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.actions_amt = actions_amt\n",
    "        self.epsilon = epsilon \n",
    "        self.batch_size = batch_size\n",
    "        self.input_shape = input_shape\n",
    "        self.epislon_dec = epsilon_dec\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.mem_size = mem_size\n",
    "        self.fname = fname\n",
    "        self.q_network = init_deep_q_network(input_shape, alpha, actions_amt, 256, 256)\n",
    "        self.q_network_target = init_deep_q_network(input_shape, alpha, actions_amt, 256, 256)\n",
    "        self.memory = Memory(input_shape, actions_amt)\n",
    "        self.loss_function = keras.losses.Huber()\n",
    "        self.optimizer = keras.optimizers.Adam(learning_rate=0.00025, clipnorm=1.0)\n",
    "\n",
    "\n",
    "    def select_action(self, input): \n",
    "        if np.random.rand() > epsilon:\n",
    "            action_values = self.q_network.predict(input)\n",
    "            action = np.argmax(action_values)\n",
    "        else:\n",
    "            action = np.random.choice(num_actions)\n",
    "        \n",
    "        return action \n",
    "    \n",
    "    def store_action(self, state, action, reward, next_state, done):\n",
    "        self.memory.save_action(state, action, reward, next_state, done)\n",
    "\n",
    "    def learn(self):\n",
    "        print(\"in learning call\")\n",
    "        states, actions, rewards, new_states, dones = self.memory.sample_memory()\n",
    "        for state in states:\n",
    "            print(\"shape of state before stack is\", np.shape(state[0]))\n",
    "        states = tf.stack(states)\n",
    "        actions = tf.stack(actions)\n",
    "        rewards = tf.stack(rewards)\n",
    "        new_states = tf.stack(new_states)\n",
    "        dones = tf.stack(dones)\n",
    "        new_states = tf.stack(new_states)\n",
    "        print(\"new states shape is\", np.shape(new_states))\n",
    "        future_rewards = self.q_network_target.predict(new_states)\n",
    "        print(\"after predicting of future rewards\")\n",
    "        updated_qs = rewards + gamma * tf.reduce_max(future_rewards, axis=1)\n",
    "        masks = tf.one_hot(actions, self.actions_amt)\n",
    "        print(\"after making masks\")\n",
    "        with tf.GradientTape() as tape:\n",
    "            q_values = self.q_network(states)\n",
    "            q_actions = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)\n",
    "            loss = self.loss_function(updated_qs, q_actions)\n",
    "        print(\"after gradient step\")\n",
    "        grads = tape.gradient(loss, self.q_network.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.q_network.trainable_variables))\n",
    "        self.q_network_target.set_weights(self.q_network.get_weights())\n",
    "\n",
    "    def save_model(self):\n",
    "        self.q_network.save(self.model_file)\n",
    "\n",
    "    def load_model(self):\n",
    "        self.q_network = load_model(self.model_file)\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Learning Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-17T03:49:44.156029Z",
     "start_time": "2023-12-17T03:49:43.747665Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in main loop, shape of observation is  (210, 160, 3) and shape of observation_ is (210, 160, 3)\n",
      "length of state in save actions is  210\n",
      "in main loop, shape of observation is  (160, 3) and shape of observation_ is (210, 160, 3)\n",
      "length of state in save actions is  160\n",
      "in main loop, shape of observation is  (160, 3) and shape of observation_ is (210, 160, 3)\n",
      "length of state in save actions is  160\n",
      "in main loop, shape of observation is  (160, 3) and shape of observation_ is (210, 160, 3)\n",
      "length of state in save actions is  160\n",
      "in main loop, shape of observation is  (160, 3) and shape of observation_ is (210, 160, 3)\n",
      "length of state in save actions is  160\n",
      "in main loop, shape of observation is  (160, 3) and shape of observation_ is (210, 160, 3)\n",
      "length of state in save actions is  160\n",
      "in main loop, shape of observation is  (160, 3) and shape of observation_ is (210, 160, 3)\n",
      "length of state in save actions is  160\n",
      "in main loop, shape of observation is  (160, 3) and shape of observation_ is (210, 160, 3)\n",
      "length of state in save actions is  160\n",
      "in main loop, shape of observation is  (160, 3) and shape of observation_ is (210, 160, 3)\n",
      "length of state in save actions is  160\n",
      "in main loop, shape of observation is  (160, 3) and shape of observation_ is (210, 160, 3)\n",
      "length of state in save actions is  160\n",
      "in learning call\n",
      "shape of state before stack is (160, 3)\n",
      "shape of state before stack is (3,)\n",
      "shape of state before stack is (3,)\n",
      "shape of state before stack is (3,)\n",
      "shape of state before stack is (3,)\n",
      "shape of state before stack is (3,)\n",
      "shape of state before stack is (3,)\n",
      "shape of state before stack is (3,)\n",
      "shape of state before stack is (3,)\n",
      "shape of state before stack is (3,)\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__Pack_N_10_device_/job:localhost/replica:0/task:0/device:CPU:0}} Shapes of all inputs must match: values[0].shape = [210,160,3] != values[1].shape = [160,3] [Op:Pack] name: stack",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mInvalidArgumentError\u001B[0m                      Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[19], line 20\u001B[0m\n\u001B[1;32m     18\u001B[0m     observation \u001B[38;5;241m=\u001B[39m observation_\n\u001B[1;32m     19\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m j \u001B[38;5;241m%\u001B[39m \u001B[38;5;241m10\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m---> 20\u001B[0m         agent\u001B[38;5;241m.\u001B[39mlearn()\n\u001B[1;32m     21\u001B[0m     j \u001B[38;5;241m=\u001B[39m j \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m     23\u001B[0m eps_history\u001B[38;5;241m.\u001B[39mappend(agent\u001B[38;5;241m.\u001B[39mepsilon)\n",
      "Cell \u001B[0;32mIn[18], line 37\u001B[0m, in \u001B[0;36mAgent.learn\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     35\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m state \u001B[38;5;129;01min\u001B[39;00m states:\n\u001B[1;32m     36\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mshape of state before stack is\u001B[39m\u001B[38;5;124m\"\u001B[39m, np\u001B[38;5;241m.\u001B[39mshape(state[\u001B[38;5;241m0\u001B[39m]))\n\u001B[0;32m---> 37\u001B[0m states \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mstack(states)\n\u001B[1;32m     38\u001B[0m actions \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mstack(actions)\n\u001B[1;32m     39\u001B[0m rewards \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mstack(rewards)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:153\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    151\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    152\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[0;32m--> 153\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    154\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    155\u001B[0m   \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/framework/ops.py:5883\u001B[0m, in \u001B[0;36mraise_from_not_ok_status\u001B[0;34m(e, name)\u001B[0m\n\u001B[1;32m   5881\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mraise_from_not_ok_status\u001B[39m(e, name) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m NoReturn:\n\u001B[1;32m   5882\u001B[0m   e\u001B[38;5;241m.\u001B[39mmessage \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m name: \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mstr\u001B[39m(name \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[0;32m-> 5883\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m core\u001B[38;5;241m.\u001B[39m_status_to_exception(e) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[0;31mInvalidArgumentError\u001B[0m: {{function_node __wrapped__Pack_N_10_device_/job:localhost/replica:0/task:0/device:CPU:0}} Shapes of all inputs must match: values[0].shape = [210,160,3] != values[1].shape = [160,3] [Op:Pack] name: stack"
     ]
    }
   ],
   "source": [
    "n_games = 20\n",
    "agent = Agent(gamma=.99, epsilon=1.0, alpha=.0005, actions_amt=4, input_shape=(210, 160, 3) , mem_size=100, batch_size=64, epsilon_end=0.01)\n",
    "\n",
    "scores = []\n",
    "eps_history = []\n",
    "\n",
    "for i in range(n_games):\n",
    "    done = False\n",
    "    score = 0\n",
    "    observation = env.reset()\n",
    "    j = 1\n",
    "    while not done: \n",
    "        action = agent.select_action(observation)\n",
    "        observation_, reward, truncated, done, info = env.step(action)\n",
    "        score += reward\n",
    "        print(\"in main loop, shape of observation is \", np.shape(observation[0]), \"and shape of observation_ is\", np.shape(observation_))\n",
    "        agent.store_action(observation[0], action, reward, observation_, done)\n",
    "        observation = observation_\n",
    "        if j % 10 == 0:\n",
    "            agent.learn()\n",
    "        j = j + 1\n",
    "\n",
    "    eps_history.append(agent.epsilon)\n",
    "    scores.append(score)\n",
    "\n",
    "    avg_score = np.mean(scores[max(0, 1 - 100):(i + 1)])\n",
    "    print(\"episode \", i, \"score %.2f\" % score, 'average score %.2f' % avg_score)\n",
    "\n",
    "    if i % 10 == 0 and i > 0: \n",
    "        agent.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Function for resizing frame"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "def resize_frame(frame):\n",
    "    frame = frame[50:-5,5:-4]\n",
    "    frame = np.average(frame,axis = 2)\n",
    "    frame = cv2.resize(frame,(84,84),interpolation = cv2.INTER_NEAREST)\n",
    "    frame = np.array(frame,dtype = np.uint8)\n",
    "    return frame"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T04:01:58.062712Z",
     "start_time": "2023-12-17T04:01:58.058184Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
